# Test Plan

**Authors:** *Aaban Vasconcelos* | **Version:** 1.0 | **Last Updated:** 10 Jun 2025

---

## Abstract

This plan documents how a project should automatically validates chatbot answers with:

1. **SBERT semantic similarity** (local, fast).
2. **LLM‑as‑a‑judge** using the bundled `llm_judge()` helper (GPT‑3.5‑turbo by default).

The repo already ships runnable pytest cases; this paper explains the theory, thresholds, and CI hooks that back them.


### LLM-as-a-Judge
LLM-as-a-Judge is a system where a large language model (LLM), like GPT, is used to help detect mistakes, errors or inconsistencies in responses generated by other AI systems.
 Think of it like a teacher grading homework from their studentes, the LLM looks at the answers and checks if they are correct or make sense.
  It works by comparing the AI's answer to a set of rules or patterns it has learned from large amounts of data and the determined patter set as a system prompt.

When an AI gives a wrong or confusing answer, the LLM-as-a-Judge can spot these errors, like when a student makes a mistake on a math problem, it’s especially useful for things like "hallucinations", which are when AI gives odd answers that sound good but are actually false, it can also be mitigated by using RAG but its not being covered here.

By using LLM-as-a-Judge, we can improve the quality of AI responses and make sure they’re reliable. It helps make sure the AI isn't just “guessing” or key words being contained in the response but giving answers that are correct, accurate and helpful but also ensuring guardrails are in place for attacks listed on OWASP for LLM Applications.

### SBERT

SBERT(Sentence-BERT) + cosine similarity is a system used to catch when a sentence doesn’t just sound different, it figures that it means something different.
SBERT takes a sentence and turns it into a vector similar to the process of tokenization, basically a list of numbers that captures the overall meaning. These vectors aren’t about grammar or exact words, they’re about what the sentence is *trying to say*. So "I love cats" and "Cats are my favorite" might look different as text, but [their vectors will be pretty close in similarity](https://colab.research.google.com/drive/1Uk08dX9alrVL0Zx5-yk6WBR61OHKalO_).

A vector should look similar to this:
```python
[ 0.01346426  0.05531251 -0.09144057  0.0785035   0.11401891
  0.06303245  0.06480928 -0.0380246  -0.10679659 -0.04623597
  0.08613613  0.08572174  0.07291689 -0.08707139 -0.05636985
  0.09612356 -0.09999207  0.08069679 -0.05298286 -0.02971558
  0.12450562  0.02970612 -0.0692575   0.09818889 -0.12569074
  0.06948257 -0.08723691  0.09097478 -0.10028882  0.05410562
 -0.0819236  -0.09713778  0.06321465 -0.0424058   0.01608018
 -0.06324141  0.0238919   0.02171907 -0.05264872  0.03527502
  0.12456086 -0.09130479 -0.00442068  0.05841394 -0.11527376 ]

```

Once we have those vectors, cosine similarity comes in. It checks how “aligned” two vectors are, much like comparing the angle between two arrows. If the arrows point in the same direction(cosine similarity near 1), it means that the meanings are similar. If they’re way off(cosine similarity closer to 0), something is probably wrong and the cutting line for it is not exactly clear, it varies with each scenario.

This combo is especially useful for detecting gross error cases where a sentence rewrite totally changes the meaning or total giggberish, for example:

- Original: "The Earth orbits the Sun."
- Rewritten: "The Sun orbits the Earth."

Both sentences use the same words, but SBERT will pick up that their vectors don’t line up. Cosine similarity drops, and we get a signal that the rewrite has a serious semantic issue.

SBERT + cosine similarity works great in grading, QA, and AI evaluation. It helps ensure the focus stays on *meaning*, not just matching keywords and can flag when an answer sounds right but isn’t.


---


## 1 Environment Setup

```bash
# Clone & install
git clone https://github.com/abanzy/QA-Engineer-Technical-Challenge-Data-Layer.git
cd llm-chatbot
pip install openai, sentence-transformers, pytest, requests
```
```bash
# Set the API key in llm_test_script.py
openai.api_key = ""
```

```bash
# Run mock_chatbot_api.py to have a mocked chatbot serveer
 python mock_chatbot_api.py

# Or use nohup for a detached 
nohup python mock_chatbot_api.py

```

```bash
# Run pytest
pytest llm_test_script.py

# Or add a -s flag for verbose in the logs
pytest -s llm_test_script.py
```

---

## 2 Test Layers & Thresholds

| Layer           | Code Path                                 | Threshold                 | Purpose                                   |
| --------------- | ----------------------------------------- | ------------------------- | ----------------------------------------- |
| **Structure**   | `run_chatbot_test()` key loop             | All required keys present | Guard schema drift                        |
| **Semantics**   | `SentenceTransformer('all-MiniLM-L6-v2')` | Cos‑sim **≥ 0.4**         | Catch gross meaning errors (matches code) |
| **LLM‑Numeric** | `llm_judge()` (1–5)                       | **≥ 4**                   | Holistic relevance/helpfulness            |
| **Edge‑cases**  | Dedicated pytest cases                    | Case by case assertions   | Ensure graceful errors                    |

---

## 3 Pass / Fail Logic

A prompt **passes** only if **every active layer** succeeds:

```text
(similarity ≥ 0.4)  AND  (judge_score ≥ 4)  AND  (all keys present)
```

Numeric fields (e.g. `totalAmount`) must match exactly when present.

---

## 4 Example Helper Function using SBERT to lock in key words and validate semantics

```python
# Happy‑path check
run_chatbot_test(
    prompt="Show me my last 5 transactions.",
    expected_keys={"transactions"},
    expected_semantics="Here are your 5 most recent transactions."
)
```

*Output*

```text
Prompt: "Show me my last 5 transactions."
Response: "Here are your 5 most recent transactions..."
SBERT similarity: 0.87
LLM judge score: 4
PASS
```

---

## 5 Bundled Pytest Cases

| Test                           | Intent                                      |
| ------------------------------ | ------------------------------------------- |
| `test_llm_last_5_transactions` | Standard list query                         |
| `test_llm_recent_txns`         | Synonym recognition                         |
| `test_llm_invalid_location`    | Out‑of‑domain prompt should trigger `error` |
| `test_llm_last_transaction`    | Single‑item fetch                           |
| `test_llm_total_amount`        | Numeric aggregation check                   |

---

## 6 Invalid & Edge‑Case Matrix

| Scenario      | Prompt                  | Expected Result                   | Mitigation                  |
| ------------- | ----------------------- | --------------------------------- | --------------------------- |
| Out‑of‑domain | "Transactions on Mars?" | `{ "error": "Invalid location" }` | `test_llm_invalid_location` |
| Ambiguous     | "recent txns"           | Returns same as canonical query   | `test_llm_recent_txns`      |
| Empty         | `""`                    | `{ "error": "Empty prompt" }`     | Input sanitation            |

---

## 7 Cost Snapshot (per call)

| Component                           | Model         | Tokens | \$ / 1k tok | \$ / call |
| ----------------------------------- | ------------- | ------ | ----------- | --------- |
| SBERT                               | Local         | n/a    | 0           | 0         |
| LLM‑judge                           | GPT‑3.5‑turbo | ≈200   | 0.002       | ≈0.0004   |

---

## 8 Why should i use LLM‑as‑a‑Judge in *my project*?

* **Cheap but reliable** – numeric rubric gives binary pass/fail via `>=4` rule.
* **Fast** – \~100 ms cloud latency; local SBERT adds < 10 ms.
* **Bias control** – prompt locked inside `llm_judge()`, versioned with code.
* **Auditability** – prints both SBERT score and judge score for every test.

Limitations: token cost, prompt sensitivity; mitigated with deterministic `temperature=0`.

---

## 9 CI Hook example

```yaml
# .github/workflows/llm-tests.yml
name: LLM QA
on: [push]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: 3.11
      - run: pip install -r requirements.txt
      - run: pytest -s llm_test_script.py
```

---

## References

1. Gu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y., Ma, S., Liu, H., Wang, Y., & Guo, J. (2024). *A Survey on LLM-as-a-Judge.* [arXiv:2411.15594v2](https://arxiv.org/html/2411.15594v2).  
   1. IDEA Research, International Digital Economy Academy, China  
   2. Institute of Computing Technology, Chinese Academy of Sciences, China  
   3. Department of Civil and Environmental Engineering, Imperial College London, UK  
   4. Gaoling School of Artificial Intelligence, Renmin University of China, China

2. Anadkat, S. (2024, October). *Custom LLM as a Judge to Detect Hallucinations with Braintrust.* [GitHub Cookbook.](https://cookbook.openai.com/examples/custom-llm-as-a-judge)  
   Shyamal Anadkat (OpenAI).

3. Reimers, N., & Gurevych, I. (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.* [arXiv:1908.10084](https://doi.org/10.48550/arXiv.1908.10084). Published at EMNLP 2019.  
   Subjects: Computation and Language (cs.CL).



---

### Appendix A – Prompt Templates

Simplified prompt chain:
```text
You are an expert assistant for evaluating chatbot responses
Given a user prompt and a chatbot's reply, rate the reply's relevance and helpfulness on a scale from 1 (poor) to 5 (excellent).
Only return the number.
```

More extensively we can add more structre for a better avaluation and finer tuning of the process:
```text
You are an expert assistant for evaluating chatbot responses
Given a user prompt and a chatbot's reply, rate the reply's relevance and helpfulness on a scale from 1 (poor) to 5 (excellent).
Only return the two numbers, separated by a comma, like 5,4 where 5 is the relevance score and 4 is the helpfulness score.
Do not return any words, labels, or explanations, only the two digits separated by a comma.
```

**Response**

```text
1‑5 or 1,1 to 5,5:
```
