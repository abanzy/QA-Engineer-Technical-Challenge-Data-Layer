# Test Plan

**Authors:** *Aaban Vasconcelos* | **Version:** 1.0 | **Last Updated:** 9 Jun 2025

---

## Abstract

This plan documents how a project should automatically validates chatbot answers with:

1. **SBERT semantic similarity** (local, fast).
2. **LLM‑as‑a‑judge** using the bundled `llm_judge()` helper (GPT‑3.5‑turbo by default).

The repo already ships runnable pytest cases; this paper explains the theory, thresholds, and CI hooks that back them.

LLM-as-a-Judge is a system where a large language model (LLM), like GPT, is used to help detect mistakes, errors or inconsistencies in responses generated by other AI systems.
 Think of it like a teacher grading homework from their studentes, the LLM looks at the answers and checks if they are correct or make sense.
  It works by comparing the AI's answer to a set of rules or patterns it has learned from large amounts of data and the determined patter set as a system prompt.

When an AI gives a wrong or confusing answer, the LLM-as-a-Judge can spot these errors, like when a student makes a mistake on a math problem, it’s especially useful for things like "hallucinations", which are when AI gives odd answers that sound good but are actually false, it can also be mitigated by using RAG but its not being covered here.

By using LLM-as-a-Judge, we can improve the quality of AI responses and make sure they’re reliable. It helps make sure the AI isn't just “guessing” or key words being contained in the response but giving answers that are correct, accurate and helpful but also ensuring guardrails are in place for attacks listed on OWASP for LLM Applications.


---


## 1 Environment Setup

```bash
# Clone & install
git clone https://github.com/your-org/myproject.git
cd myproject
pip install -r requirements.txt  # openai, sentence-transformers, pytest, requests

# Keys
export OPENAI_API_KEY="<your key>"
```

*Optional*: disable pytest noise:

```ini
[pytest]
addopts = -q --maxfail=1 --disable-warnings
```

---

## 2 Test Layers & Thresholds

| Layer           | Code Path                                 | Threshold                 | Purpose                                   |
| --------------- | ----------------------------------------- | ------------------------- | ----------------------------------------- |
| **Structure**   | `run_chatbot_test()` key loop             | All required keys present | Guard schema drift                        |
| **Semantics**   | `SentenceTransformer('all-MiniLM-L6-v2')` | Cos‑sim **≥ 0.4**         | Catch gross meaning errors (matches code) |
| **LLM‑Numeric** | `llm_judge()` (1–5)                       | **≥ 4**                   | Holistic relevance/helpfulness            |
| **Edge‑cases**  | Dedicated pytest cases                    | n/a                       | Ensure graceful errors                    |

---

## 3 Pass / Fail Logic

A prompt **passes** only if **every active layer** succeeds:

```text
(similarity ≥ 0.4)  AND  (judge_score ≥ 4)  AND  (all keys present)
```

Numeric fields (e.g. `totalAmount`) must match exactly when present.

---

## 4 Example Helper Function

```python
from myproject.tests import run_chatbot_test

# Happy‑path check
run_chatbot_test(
    prompt="Show me my last 5 transactions.",
    expected_keys={"transactions"},
    expected_semantics="Here are your 5 most recent transactions."
)
```

*Output*

```text
Prompt: Show me my last 5 transactions.
Response: Here are your 5 most recent transactions...
SBERT similarity: 0.87
LLM judge score: 4
PASS
```

---

## 5 Bundled Pytest Cases

| Test                           | Intent                                      |
| ------------------------------ | ------------------------------------------- |
| `test_llm_last_5_transactions` | Standard list query                         |
| `test_llm_recent_txns`         | Synonym recognition                         |
| `test_llm_invalid_location`    | Out‑of‑domain prompt should trigger `error` |
| `test_llm_last_transaction`    | Single‑item fetch                           |
| `test_llm_total_amount`        | Numeric aggregation check                   |

Run all with:

```bash
pytest -q
```

---

## 6 Invalid & Edge‑Case Matrix

| Scenario      | Prompt                  | Expected Result                   | Mitigation                  |
| ------------- | ----------------------- | --------------------------------- | --------------------------- |
| Out‑of‑domain | "Transactions on Mars?" | `{ "error": "Invalid location" }` | `test_llm_invalid_location` |
| Ambiguous     | "recent txns"           | Returns same as canonical query   | `test_llm_recent_txns`      |
| Empty         | `""`                    | `{ "error": "Empty prompt" }`     | Input sanitation            |

---

## 7 Cost Snapshot (per call)

| Component                           | Model         | Tokens | \$ / 1k tok | \$ / call |
| ----------------------------------- | ------------- | ------ | ----------- | --------- |
| SBERT                               | Local         | n/a    | 0           | 0         |
| LLM‑judge                           | GPT‑3.5‑turbo | ≈200   | 0.002       | ≈0.0004   |

---

## 8 Why LLM‑as‑a‑Judge in *myproject*?

* **Cheap but reliable** – numeric rubric gives binary pass/fail via `>=4` rule.
* **Fast** – \~100 ms cloud latency; local SBERT adds < 10 ms.
* **Bias control** – prompt locked inside `llm_judge()`, versioned with code.
* **Auditability** – prints both SBERT score and judge score for every test.

Limitations: token cost, prompt sensitivity; mitigated with deterministic `temperature=0`.

---

## 9 CI Hook

```yaml
# .github/workflows/llm-tests.yml
name: LLM QA
on: [push]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: 3.11
      - run: pip install -r requirements.txt
      - run: pytest -q
```

---

## References

1. Gu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y., Ma, S., Liu, H., Wang, Y., & Guo, J. (2024). *A Survey on LLM-as-a-Judge.* [arXiv:2411.15594v2](https://arxiv.org/html/2411.15594v2).  
   1. IDEA Research, International Digital Economy Academy, China  
   2. Institute of Computing Technology, Chinese Academy of Sciences, China  
   3. Department of Civil and Environmental Engineering, Imperial College London, UK  
   4. Gaoling School of Artificial Intelligence, Renmin University of China, China

2. Anadkat, S. (2024, October). *Custom LLM as a Judge to Detect Hallucinations with Braintrust.* GitHub Cookbook. [Link to GitHub](https://github.com)  
   Shyamal Anadkat (OpenAI).

3. Reimers, N., & Gurevych, I. (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.* [arXiv:1908.10084](https://doi.org/10.48550/arXiv.1908.10084). Published at EMNLP 2019.  
   Subjects: Computation and Language (cs.CL).



---

### Appendix A – Prompt Templates

**System**

```text
You are an expert assistant for evaluating chatbot responses…
```

**User**

```text
User prompt: {prompt}
Chatbot reply: {reply}
Score (1‑5):
```